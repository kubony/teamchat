# pages/7_ğŸ‘¥_multi_agent_chat.py
import os
import json
import utils
import streamlit as st
from streaming import StreamHandler
from langchain.chains import ConversationChain
from langchain.memory import ConversationBufferMemory
from langchain.schema import HumanMessage, SystemMessage
from loguru import logger
from config.settings import settings
import random
from anthropic import Anthropic

st.set_page_config(page_title="ë‹¤ì¤‘ AI ì—ì´ì „íŠ¸ ì±„íŒ…", page_icon="ğŸ’¬")
st.header('ë‹¤ì¤‘ AI ì—ì´ì „íŠ¸ ì±„íŒ…')
st.write('ì—¬ëŸ¬ AI ì—ì´ì „íŠ¸ê°€ ì°¸ì—¬í•˜ëŠ” ë‹¨ì²´ ì±„íŒ…ë°©ì…ë‹ˆë‹¤.')

class MultiAgentChat:
    def __init__(self):
        utils.sync_st_session()
        self.agents = {}  # ì—¬ê¸°ì„œ self.agentsë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
        self.agents = self.setup_agents()
        self.moderator = self.setup_moderator()
        self.baton = 0
        self.conversation_history = []
        self.log_file = 'agent_interactions.jsonl'

    def setup_agents(self):
        agents = {}
        for agent_name, agent_info in settings.AGENTS.items():
            if agent_name != "moderator":
                agents[agent_name] = {
                    'context': self.load_role_context(agent_name, agent_info['role']),
                    'memory': ConversationBufferMemory(max_token_limit=2000),
                    'chain': self.setup_chain(agent_info['model'], agent_name)
                }
        return agents
    
    def load_role_context(self, role, role_description):
        context = f"ë‹¹ì‹ ì˜ ì—­í• ì€ {role}ì…ë‹ˆë‹¤. {role_description}\n\n"
        path = os.path.join(os.getcwd(), 'role_contexts', f"{role}.md")
        if os.path.exists(path):
            with open(path, 'r', encoding='utf-8') as file:
                context += file.read()
        return context

    def setup_chain(self, model, agent_name):
        if model.startswith('claude-'):
            return Anthropic(api_key=settings.ANTHROPIC_API_KEY.get_secret_value())
        else:
            llm = utils.configure_llm_with_model(model)
            memory = ConversationBufferMemory(max_token_limit=2000)
            chain = ConversationChain(llm=llm, memory=memory, verbose=False)
            return chain

    def setup_moderator(self):
        moderator_info = settings.AGENTS["moderator"]
        llm = utils.configure_llm_with_model(moderator_info['model'])
        memory = ConversationBufferMemory(max_token_limit=2000)
        return ConversationChain(llm=llm, memory=memory, verbose=False)
    
    def get_next_speaker(self):
        agent_names = [name for name in settings.AGENTS.keys() if name != "moderator"]
        moderator_input = f"ëŒ€í™” ë‚´ìš©: {self.get_conversation_history_string()}\n\në‹¤ìŒ ë°œì–¸ìë¥¼ {', '.join(agent_names)} ì¤‘ì—ì„œ ì„ íƒí•˜ì„¸ìš”. ì„ íƒí•œ ì—­í• ì˜ ì´ë¦„ë§Œ ë‹µë³€ìœ¼ë¡œ ì œì‹œí•˜ì„¸ìš”."
        response = self.moderator.invoke({"input": moderator_input})
        next_speaker = response['response'].strip().lower()
        
        if next_speaker not in agent_names or next_speaker == self.conversation_history[-1].split(':')[0]:
            logger.warning(f"Invalid speaker selected: {next_speaker}. Selecting randomly from others.")
            available_speakers = [name for name in agent_names if name != self.conversation_history[-1].split(':')[0]]
            return random.choice(available_speakers)
        
        return next_speaker
    
    def log_interaction(self, role, prompt, response):
        log_entry = {
            "role": role,
            "prompt": prompt,
            "response": str(response)  # ë¬¸ìì—´ë¡œ ë³€í™˜
        }
        with open(self.log_file, 'a', encoding='utf-8') as f:
            json.dump(log_entry, f, ensure_ascii=False)
            f.write('\n')

    def add_to_conversation_history(self, message):
        self.conversation_history.append(message)
        # ëª¨ë“  ì—ì´ì „íŠ¸ì˜ ë©”ëª¨ë¦¬ì— ë©”ì‹œì§€ ì¶”ê°€
        for agent in self.agents.values():
            agent['memory'].chat_memory.add_user_message(message)

    def get_conversation_history_string(self):
        return "\n".join(self.conversation_history[-10:])  # ìµœê·¼ 10ê°œì˜ ë©”ì‹œì§€ë§Œ í¬í•¨

    @utils.enable_chat_history
    def main(self):
        user_query = st.chat_input(placeholder="ëŒ€í™”ë¥¼ ì‹œì‘í•˜ì„¸ìš”!")
        
        if user_query:
            self.add_to_conversation_history(f"User: {user_query}")
            utils.display_msg(user_query, 'user')
            self.baton = 1
            logger.info(f"ëŒ€í™” ì‹œì‘: ë°”í†¤ ì¹´ìš´íŠ¸ {self.baton}", extra={"action": "start_conversation"})
            
            while self.baton > 0:
                next_speaker = self.get_next_speaker()
                
                with st.chat_message("assistant"):
                    st_cb = StreamHandler(st.empty())
                    try:
                        agent = self.agents[next_speaker]
                        conversation_history = self.get_conversation_history_string()
                        full_query = f"""ì—­í•  ì»¨í…ìŠ¤íŠ¸:\n{agent['context']}\n\n
ëŒ€í™” ë‚´ìš©:\n{conversation_history}\n\n
ë‹¹ì‹ ì€ {next_speaker} ì—­í• ì…ë‹ˆë‹¤. ìœ„ì˜ ëŒ€í™” ë‚´ìš©ì„ ëª¨ë‘ ê³ ë ¤í•˜ì—¬ ë‹¤ìŒ ë°œì–¸ì„ í•´ì£¼ì„¸ìš”. ë‹¤ë¥¸ ì—ì´ì „íŠ¸ë“¤ì˜ ì˜ê²¬ë„ ê³ ë ¤í•˜ì„¸ìš”:"""
                        
                        if isinstance(agent['chain'], Anthropic):
                            # Claude ëª¨ë¸ ì‚¬ìš©
                            logger.debug("Using Anthropic API for agent: %s", next_speaker)
                            message = agent['chain'].messages.create(
                                model="claude-3-5-sonnet-20240620",
                                max_tokens=1000,
                                temperature=0,
                                system=agent['context'],
                                messages=[
                                    {"role": "user", "content": conversation_history},
                                    {"role": "user", "content": full_query}
                                ]
                            )
                            response = message.content[0].text if isinstance(message.content, list) else message.content
                            st_cb.on_llm_new_token(response)
                        else:
                            # ë‹¤ë¥¸ ëª¨ë¸ ì‚¬ìš©
                            logger.debug("Using other model for agent: %s", next_speaker)
                            result = agent['chain'].invoke(
                                {"input": full_query},
                                {"callbacks": [st_cb]}
                            )
                            response = result["response"]

                        response_str = str(response)
                        
                        st.markdown(f"**{next_speaker}**: {response_str}")
                        
                        self.add_to_conversation_history(f"{next_speaker}: {response_str}")
                        logger.info(f"{next_speaker} ì‘ë‹µ: {utils.truncate_string(response_str)}", extra={"action": "agent_response", "agent": next_speaker})
                        
                        self.log_interaction(next_speaker, utils.truncate_string(full_query), utils.truncate_string(response_str))
                        
                        self.baton -= 1
                        logger.info(f"ë°”í†¤ ì¹´ìš´íŠ¸ ê°ì†Œ: {self.baton}", extra={"action": "decrease_baton"})
                    
                    except Exception as e:
                        error_msg = f"ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
                        st.error(error_msg)
                        logger.error(f"ìƒì„¸ ì˜¤ë¥˜ ì •ë³´: {type(e).__name__}, {str(e)}", extra={"action": "error"})
                        logger.exception("ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤:")
                        break
            
            logger.info(f"ëŒ€í™” ì¢…ë£Œ: ë°”í†¤ ì¹´ìš´íŠ¸ {self.baton}", extra={"action": "end_conversation"})
            
if __name__ == "__main__":
    obj = MultiAgentChat()
    obj.main()